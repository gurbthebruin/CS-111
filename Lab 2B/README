NAME: Gurbir Arora
EMAIL: gurbthebruin@g.ucla.edu
ID: 105178554

Included files: 

lab2_list.c: the source for a C program that compiles cleanly (with no errors or warnings), and implements the specified command line options (--threads, --iterations, --yield, --sync, --lists), drives one or more parallel threads that do operations on a shared linked list, and reports on the final list and performance. Note that we expect segmentation faults in non-synchronized multi-thread runs, so your program should catch those and report the run as having failed.

Makefile: to build the deliverable programs, output, graphs, and tarball. For your early testing you are free to run your program manually, but by the time you are done, all of the below-described test cases should be executed, the output captured, and the graphs produced automatically. The higher level targets should be:
default ... the lab2_list executable (compiling with the -Wall and -Wextra options).
tests ... run all specified test cases to generate CSV results
profile ... run tests with profiling tools to generate an execution profiling report
graphs ... use gnuplot to generate the required graphs
dist ... create the deliverable tarball
clean ... delete all programs and output generated by the Makefile

lab2b_list.csv - containing your results for all of test runs.

profile.out - execution profiling report showing where time was spent in the un-partitioned spin-lock implementation.

graphs (.png files), created by gnuplot(1) on the above csv data showing:
	lab2b_1.png ... throughput vs. number of threads for mutex and spin-lock synchronized list operations.
	lab2b_2.png ... mean time per mutex wait and mean time per operation for mutex-synchronized list operations.
	lab2b_3.png ... successful iterations vs. threads for each synchronization method.
	lab2b_4.png ... throughput vs. number of threads for mutex synchronized partitioned lists.
	lab2b_5.png ... throughput vs. number of threads for spin-lock-synchronized partitioned lists.

README: file containing:
descriptions of each of the included files and any other information about your submission that you would like to bring to our attention (e.g. research, limitations, features, testing methodology).
brief (a few sentences per question) answers to each of the questions (below).

profile.sh: file containing instruction to support make profile 

Questions: 

QUESTION 2.3.1 - CPU time in the basic list implementation:
Where do you believe most of the CPU time is spent in the 1 and 2-thread list tests ?

Why do you believe these to be the most expensive parts of the code?

Where do you believe most of the CPU time is being spent in the high-thread spin-lock tests?

Where do you believe most of the CPU time is being spent in the high-thread mutex tests?


Most of the cpu time is spent on the list operations for the 1 thread test, while sometime is spent on locking during the 2 thread test, but still a good amount of time is spent on list operations for both. When only 1 or 2 threads are present, then there's not much of a wait for locks. During the high-thread spin lock tests, most of the time is spent spinning. In the high-thread mutex tests, most of the time is spent on the mutex functions. 

For the 1 and 2-thread lists, when there's 1 thread and a mutex, most of the time is spent on list operations since not much time is spent on waiting for locks. 
For when there's 1 thread and a spin lock, the same thing appears to be true where most of the time is spent on performing list ooeprations. 
When there's 2 threads and a mutex, most of the time goes to list operations for larger lists because not much time is required to for locking and unlocking. Secondly, 
where there's 2 threads and a spin lock, the time taken for list operations and locking is about the same as once one thread is doing list operations, the other will 
be spinning and vice versa. 

List operations and locking seem to be the most expensive parts of the code due to the fact that they are run by all threads. 

QUESTION 2.3.2 - Execution Profiling:
Where (what lines of code) are consuming most of the CPU time when the spin-lock version of the list exerciser is run with a large number of threads?

Why does this operation become so expensive with large numbers of threads?

The while loop that checks for the spin lock consumes the most amount of time. This operation gets more and more costly due to the fact that each time a thread is scheduled, it must spin. 

This while loop is the most expensive with large numbers of threads because this loop will try to grab the spin lock for all the threads that cannot get the lock. Since this is done repetitively and is
depedent on the contest for resources, as the number of threads increase, the contest for resources increases and this operation becomes more expensive. 

QUESTION 2.3.3 - Mutex Wait Time:
Look at the average time per operation (vs. # threads) and the average wait-for-mutex time (vs. #threads).
Why does the average lock-wait time rise so dramatically with the number of contending threads?
Why does the completion time per operation rise (less dramatically) with the number of contending threads?
How is it possible for the wait time per operation to go up faster (or higher) than the completion time per operation?

The average lock-wait time rises so dramatically with the number of contending threads because more and more threads are waiting to attain the lock and since the mutex operations must be ran in order to check if the lock is available, the averagelock-wait time rises dramatically. Basically, more threads are competing for the same number of resources, which is why ths rise is dramatic. 

The completion per time operation rises less dramatically with the number of contending threads because there are more threads that need to perform list operations and therefore the number of context switches increases. These context swithes require setup time in order to be completed. 

The wait time per operation could go up faster than the completion time per operation due to the fact that each thread has its own wait time timer, while the completion timer is the same for all threads. This could create an double counted time when calculating the wait time while the compeltetion timer would have this double count, resulting in this. 

QUESTION 2.3.4 - Performance of Partitioned Lists
Explain the change in performance of the synchronized methods as a function of the number of lists.
Should the throughput continue increasing as the number of lists is further increased? If not, explain why not.
It seems reasonable to suggest the throughput of an N-way partitioned list should be equivalent to the throughput of a single list with fewer (1/N) threads. Does this appear to be true in the above curves? If not, explain why not.

The relationship between synchronized methods as a function of the number of lists is, as the number of lists increases, the performance increases meaning that the throughput increases. This is because more parallelism is possible with an increasing number of lists, meaning that the performance will increase. However, as the number of lists approaches infinity, there will be a point where throughput can no longer increase. 

This assumption doesn't seem to be true from what the graphs suggest, as a list partitioned N times would spend more time locking than say a single list with 1/N times the amount of threads, thus the throughput of an N-way partitioned list is not necessarily quivalent to the throughput of a single list with fewer (1/N) threads. 
